{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhQqwXjRPQzdkZZ2O9c1W6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to torch.autograd\n","\n","`torch.autograd` is PyTorch's automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train."],"metadata":{"id":"3g3UJFxtat0M"}},{"cell_type":"markdown","source":["## Background\n","\n","Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n","\n","Training a NN happens in two steps:\n","\n","**Forward Propagation:** In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n","\n","**Backward Propagation:** In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent."],"metadata":{"id":"Un5KEu1EbJ3g"}},{"cell_type":"markdown","source":["## Usage in PyTorch\n","\n","Let's take a look at a single training step. For this example, we load a pretrained resnet18 model from `torchvision`. We create a random data tensor to represent a single image with 3 channels, and height & width of 64, and its corresponding `label` initialized to some random values. Label in pretrained models has shape (1,1000)."],"metadata":{"id":"hky-s0qqcs_D"}},{"cell_type":"code","source":["import torch\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","model = resnet18(weights=ResNet18_Weights.DEFAULT)\n","data = torch.rand(1, 3, 64, 64)\n","labels = torch.rand(1, 1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmuQU7ywbfTW","executionInfo":{"status":"ok","timestamp":1730275577602,"user_tz":-180,"elapsed":923,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}},"outputId":"6e67283d-0b9f-4b0c-ea05-4722592e389f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 136MB/s]\n"]}]},{"cell_type":"code","source":["print(\"Data tensor info: \\n\")\n","print(f\"Data tensor shape: {data.shape} \\n\")\n","print(f\"Data tensor type: {data.dtype} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j953pLcNdgZb","executionInfo":{"status":"ok","timestamp":1730275682642,"user_tz":-180,"elapsed":458,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}},"outputId":"23a60569-4d61-4974-aee9-ab4616e049bb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Data tensor info: \n","\n","Data tensor shape: torch.Size([1, 3, 64, 64]) \n","\n","Data tensor type: torch.float32 \n","\n"]}]},{"cell_type":"markdown","source":["This tensor is 4D.\n","\n","Here's a breakdown of the dimensions in the shape `torch.Size([1, 3, 64, 64])`:\n","\n","1. **1**: The first dimension typically represents the *batch size*, indicating that there's one sample in this batch.\n","2. **3**: The second dimension often represents the *channels*, such as RGB channels in an image.\n","3. **64**: The third dimension is the *height* of the data (in this case, an image).\n","4. **64**: The fourth dimension is the *width* of the data (image width).\n","\n","So, `[1, 3, 64, 64]` represents a batch of one 64x64 RGB image (3 channels), making it a 4D tensor with dimensions `[Batch, Channels, Height, Width]`."],"metadata":{"id":"rnFqGq11eIpE"}},{"cell_type":"markdown","source":["Next, we run the input data through the model through each of its layers to make a prediction. This is the forward pass.\n","\n"],"metadata":{"id":"3r6yRsXLeS9n"}},{"cell_type":"code","source":["prediction = model(data) # forward pass"],"metadata":{"id":"AAmGWt91eSz0","executionInfo":{"status":"ok","timestamp":1730276051259,"user_tz":-180,"elapsed":296,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["We use the model's prediction and the corresponding label to calculate the error (`loss`). The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call `.backward()` on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter's `.grad` attribute."],"metadata":{"id":"rGSxP0hFekv1"}},{"cell_type":"code","source":["loss = (prediction - labels).sum()\n","loss.backward() # backward pass"],"metadata":{"id":"63rDeQKofFxT","executionInfo":{"status":"ok","timestamp":1730276052443,"user_tz":-180,"elapsed":302,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Next, we load an optimizer, in this case SGD(Stochastic Gradient Descent) with a learning rate of 0.01 and momentum of 0.9. We register all the parameters of the model in the optimizer.\n","\n"],"metadata":{"id":"rat87g_AfV4q"}},{"cell_type":"code","source":["optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"],"metadata":{"id":"ImGbN3fufVt7","executionInfo":{"status":"ok","timestamp":1730276673092,"user_tz":-180,"elapsed":317,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Finally, we call `.step()` to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in `.grad`.\n","\n"],"metadata":{"id":"NiKhUJqXhs0M"}},{"cell_type":"code","source":["optim.step() # gradient descent"],"metadata":{"id":"BXcIvMVbhsNf","executionInfo":{"status":"ok","timestamp":1730276709651,"user_tz":-180,"elapsed":326,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Differentiation in Autograd\n"],"metadata":{"id":"sRUX6stUj3KF"}},{"cell_type":"markdown","source":["Let's take a look at how autograd collects gradients. We create two tensors a and b with `requires_grad=True`. This signals to `autograd` that every operation on them should be tracked."],"metadata":{"id":"qqXAASdNj9Sg"}},{"cell_type":"code","source":["import torch\n","\n","a = torch.tensor([2., 3.], requires_grad=True)\n","b = torch.tensor([6., 4.], requires_grad=True)"],"metadata":{"id":"DaCMPhXgkIhS","executionInfo":{"status":"ok","timestamp":1730277356275,"user_tz":-180,"elapsed":712,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["We create another tensor `Q` from `a` and `b`.\n","\n","$Q = 3a^3 - b^2$\n"],"metadata":{"id":"wMw6LcblkTTT"}},{"cell_type":"code","source":["Q = 3 * a ** 3 - b ** 2"],"metadata":{"id":"0vOq2D2okrhf","executionInfo":{"status":"ok","timestamp":1730277470858,"user_tz":-180,"elapsed":321,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Let's assume `a` and `b` are parameters of an NN, and Q to be the error. In NN training, we want gradients of the error w.r.t. parameters, i.e.\n","\n","\n","$\\frac{\\partial Q}{\\partial a} = 9a^2$\n","\n","$\\frac{\\partial Q}{\\partial b} = -2b$\n","\n","\n","When we call `.backward()` on Q, autograd calculates these gradients and stores them in the respective tensors' `.grad` attribute.\n","\n","We need to explicitly pass a `gradient` argument in `Q.backward()` because it is a vector. `gradient` is a tensor of the same shape as `Q`, and it represents the gradient of Q w.r.t. itself, i.e.\n","\n","$\\frac{\\partial Q}{\\partial Q} = 1$\n","\n","\n","Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like `Q.sum().backward()`.\n","\n"],"metadata":{"id":"Xb9OBkw5kvfK"}},{"cell_type":"code","source":["external_grad = torch.tensor([1, 1])\n","Q.backward(gradient=external_grad)"],"metadata":{"id":"-5L46E4ylPpD","executionInfo":{"status":"ok","timestamp":1730278119918,"user_tz":-180,"elapsed":325,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Tabii ki!\n","\n","Burada \\( Q \\), bir hata fonksiyonu gibi düşünebiliriz ve diyelim ki \\( Q \\), yapay sinir ağı parametreleri \\( a \\) ve \\( b \\) ile hesaplanıyor. Amacımız, hatanın \\( a \\) ve \\( b \\) parametrelerine göre türevini (gradyanını) bulmak. Bu da sinir ağı eğitiminde önemli çünkü ağ, hatayı azaltmak için gradyanları kullanarak parametreleri günceller.\n","\n","### Neden \\([1, 1]\\) Gradyan Kullanıyoruz?\n","\n","Eğer \\( Q \\) bir vektör veya birden fazla elemandan oluşuyorsa, PyTorch'un gradyanı doğru hesaplayabilmesi için başta bir dış gradyan tanımlamamız gerekiyor. Bu dış gradyanı şöyle tanımlıyoruz:\n","\n","```python\n","external_grad = torch.tensor([1., 1.])\n","Q.backward(gradient=external_grad)\n","```\n","\n","#### Açıklaması:\n","1. **Başlangıç Gradyanı Sağlamak**: Burada \\([1, 1]\\), \\( Q \\)'nun her bir bileşenine göre kendisinin gradyanı olarak kabul edilir. Yani, bu $(\\frac{dQ}{dQ} = [1, 1])$ demektir. Bu başlangıç gradyanı, parametreler \\( a \\) ve \\( b \\)'ye doğru düzgün bir şekilde yayılır ve sonuçta PyTorch, \\( Q \\)'ya göre \\( a \\) ve \\( b \\)'nin gradyanını doğru bir şekilde hesaplayabilir.\n","\n","2. **Çoklu Bileşenler için Gerekli**: Eğer \\( Q \\) iki elemanlı bir vektörse (örneğin $( Q = [Q_1, Q_2] )$ gibi), PyTorch hangi bileşene göre hesaplama yapacağını bilmez. Bu durumda, \\([1, 1]\\) kullanarak her bileşenin bağımsız olduğunu ve kendine göre \\(1\\) gradyana sahip olduğunu belirtmiş oluruz.\n","\n","3. **Tekil Skalar Değilse Gerekli**: PyTorch'ta `.backward()` sadece tek bir skalar (tekil sayı) üzerinde çalışabilir. Eğer \\( Q \\) skalar değil de çok bileşenli bir yapıdaysa, dışarıdan bir gradyan vermezsek PyTorch bunu işlemez. Bu yüzden \\([1, 1]\\) gibi bir gradyanla her bir bileşeni başlatmamız gerekir.\n","\n","Bu, PyTorch'un otomatik türev alma sisteminde hatanın doğru bir şekilde geriye yayılmasını sağlıyor, ve sonuç olarak model parametreleri (mesela \\( a \\) ve \\( b \\)) doğru şekilde güncelleniyor.\n","\n","---\n","\n","Burada asıl mesele **her bir \\( Q \\) bileşeninin geriye yayılma sırasında ne kadar ağırlıklandırılacağı** ve **inputların gradyanlarının nasıl etkilenmesini istediğimiz** ile ilgilidir.\n","\n","### Özetle:\n","1. **Ağırlıklandırma (Weighting)**: \\([1, 6]\\) gibi bir gradyan vererek, her bileşene farklı önem veya ağırlık verebiliyoruz. Yani, \\( Q \\)'nun bileşenlerinin katkısının eşit veya farklı olmasını bu sayede kontrol ediyoruz.\n","2. **Geriye Yayılmada Etki**: Eğer \\( Q \\)'nun bazı bileşenlerinin model parametrelerine daha fazla etki etmesini istiyorsak, o bileşenlere daha yüksek bir değer atayarak bunu sağlarız. Mesela \\([1, 6]\\) ile ikinci bileşene daha fazla önem vermiş oluyoruz.\n","3. **Bağımsızlık Durumu**: Bu ağırlıklandırma, inputların veya bileşenlerin bağımsız olup olmadığını değil, her bir bileşenin gradyan hesabındaki etkisini değiştirmek için kullanılır. Bu etki, geriye yayılma sırasında parametre güncellemelerini etkileyen bir faktördür.\n","\n","Yani, burada gradyanın ağırlıklandırılması önemli olan nokta ve bu, modelin her bileşene göre farklı hassasiyetlerde güncellenmesini sağlıyor."],"metadata":{"id":"J94mJubMox8b"}},{"cell_type":"markdown","source":["Gradients are now deposited in `a.grad` and `b.grad`.\n","\n","\n","\n"],"metadata":{"id":"U2ZruaH2nP1y"}},{"cell_type":"code","source":["# check if collected gradients are correct\n","\n","print(9 * a ** 2 == a.grad)\n","\n","print(-2 * b == b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiiipUVqnOlw","executionInfo":{"status":"ok","timestamp":1730278275323,"user_tz":-180,"elapsed":312,"user":{"displayName":"Emin Üstün","userId":"00082162531249404939"}},"outputId":"84275e41-4fb0-4bf7-faba-9b793be99f06"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([True, True])\n","tensor([True, True])\n"]}]},{"cell_type":"markdown","source":["### Devamında bir sürü bölüm var ama not alması zor yerler. Link: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"],"metadata":{"id":"6IvARNfUsBfg"}}]}